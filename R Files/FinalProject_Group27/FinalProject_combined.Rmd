---
title: "Predicting the Success of Bank Telemarketing"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# STA 141 Final Project

### Group 27: Sameerah Helal, Mason Del Rio, Kaiming Fu, Yulu Jin

## Introduction
### Background
Customer acquisition and retention are some of most important goals of any business. This is particularly true of banks, which rely on long-term connections, like giving loans or hosting bank accounts, in order to stay in business. We are given a data set related to a marketing campaign performed by a Portuguese bank. The data includes features of client details and social/economic context variables, with the variable of interest being campaign success, defined by whether or not a client subscribed to a long-term deposit with the bank.\
Given this data, our primary research goal is to identify the best model that will predict whether or not  a given client will subscribe to a long term deposit. We will attempt to find the most important individual features as well as their most useful combinations and linear combinations. We will then test different prediction models, tuning to get the best parameters, then comparing the models to identify the one best suited to predict our variable of interest.

### Literature Review
We have done literature review about support vector machines(SVM) and outlier computation. \

SVM was proposed by Vapnik and his fellows in the 1990s and was applied to analyze data used for classification and regression analysis. Given a set of training examples, a classification model that assigns examples into different categories is generated by mapping the input data into a higher dimensional space and constructing an optimal separating hyper-plane,\
<https://link.springer.com/article/10.1023/A:1018628609742>\
Given a data set, outliers are data points that deviate from the rest of the samples, often
to the point of skewing any models trained using them. There are many methods for computing outliers, including, but not limited to z-score, local outlier factor, one-class SVM, and isolation forest. For our data, we use isolation forest because it is suited for data with many, sometimes irrelevant attributes; and it is robust despite requiring few parameters.\
Isolation forest is a non-parametric, unsupervised outlier detection algorithm that is principled upon outliers being both few in number and far from the rest of the data. As a method that uses binary trees, the idea of isolation forest is that any anomalies in the data would be more easily "isolated" into leaf nodes than the other data points. The measure of ease of isolation, in this case, is height: path-length from the root. The algorithm randomly selects a feature and a split value within the min-max range, and as it compares observations to the split value during prediction, “path lengths” are recorded. Those points that are isolated faster, outliers, will have shorter path lengths. For each observation, an outlier score in the unit range $[0,1]$ is computed, corresponding to the sample's "outlierness", with 1 being more like an outlier and 0 being less like an outlier.\
<https://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/icdm08b.pdf?q=isolation-forest>

## Proposed methods
In this section, we perform logistic regression, random forest and SVM for classification on the "bank-additional-full.csv" dataset. Cross validation is also performed for each algorithm when it applies. 

### Logistic Regression and LASSO Regularization
In order to select which features matter the most, we will use Lasso Regularization on our Logistic Regression Model.\

#### Logistic Regression

The equation for Logistic Regression is:

$P=\frac{e^{{\beta}_0+{\beta}_1X}}{1+e^{{\beta}_0+{\beta}_1X}}$

Where P is our target variable which varies from 0 to 1, and the ${\beta}_0$, ${\beta}_1$, and $X$ are the independent variables. 

Or similarly, $log(\frac{p_i}{1-p_i}) = {\beta}_0 + {\beta}_1X_1 + {\beta}_2X_2...{\beta}_kX_k$
Where $log(\frac{p_i}{1-p_i})$ is the Log Likelihood. 


We first index the data into a Training Set and a Testing Set using the sample function.
```{r include=FALSE}
library(glmnet)
library(tidyr)
library(ggplot2)
library(data.table)
library(gbm)   
library(class)
library(dplyr)

set.seed(1)
#Load Data
bank.data = read.csv("/Users/yulujin/Dropbox/STA141/bank-additional-full.csv", sep=";")
bank.data <- bank.data %>% mutate(y = ifelse(as.character(y) =="yes", 1, 0))
index = sample(1:nrow(bank.data), 0.7*nrow(bank.data))
train = bank.data[index,] 
test = bank.data[-index,] 
dim(train)
dim(test)
```
As we can see, 70% of our data was split for indexing to our training and testing data. 

28831/41188 = 0.69999854

We now have our data split up, randomly, most importantly, and we can fit our logistic model to the training data using the glm() function.

```{r include=FALSE}

glm_model <- glm(y~., family = binomial, data = train)
glm_model

```
In order to see how accurate our model is, we need to set up a confusion matrix.
We use the test data that was separate from the training model in order to simulate some form of real world implementation and feed it into the predict.glm() function. 
```{r include=FALSE}
glm_prob <- predict.glm(glm_model, test, type = "response")
glm_predict<- rep("no", nrow(test))
glm_predict[glm_prob>.5] <- "yes"
table(pred=glm_predict, true = test$y)
mean(glm_predict == test$y)


```
The Logistic Regression model works fairly well for the test data, with 91.3% accuracy. This is a model that works well with the data that was randomized and set once, for training and testing purposes. However, we must use K-Fold Cross Validation in order to test the model's ability to predict outside data not used in the process of creating this model. 

#### Logistic Regression with K-Fold Cross Validation

We will use the caret library to implement cross validation.
Below, we will again create a training and testing set for the data. However, this time the K-Fold Cross Validation will randomly select portions of the training data 10 times. 
```{r include=FALSE}
bank = read.csv("bank-additional-full.csv", sep = ";")
#Install caret function

library(caret)
require(dplyr)

#Partition Data: Create index matrix of selected values
set.seed(1)


# Create index matrix
bank = na.omit(bank)
# data preparation
bank <- bank %>% mutate(marital = ifelse(as.character(marital) =="married", 1, 0))
bank <- bank %>% mutate(housing = ifelse(as.character(housing) =="yes", 1, 0))
bank <- bank %>% mutate(loan = ifelse(as.character(loan) =="yes", 1, 0))
bank <- bank %>% mutate(y = ifelse(as.character(y) =="yes", 1, 0))
bank$job <- as.numeric(factor(bank$job, levels = as.character(unique(bank$job))))
bank$education <- as.numeric(factor(bank$education, levels = as.character(unique(bank$education))))
bank$default <- as.numeric(factor(bank$default, levels = as.character(unique(bank$default))))
bank$contact <- as.numeric(factor(bank$contact, levels = as.character(unique(bank$contact))))
bank$month <- as.numeric(factor(bank$month, levels = as.character(unique(bank$month))))
bank$day_of_week <- as.numeric(factor(bank$day_of_week, levels = as.character(unique(bank$day_of_week))))
bank$poutcome <- as.numeric(factor(bank$poutcome, levels = as.character(unique(bank$poutcome))))

#Don't want list, we get a matrix. 
index <- createDataPartition(bank$y, p = .7, list = FALSE, times = 1)

#Convert Dataframe bank to actual df
bank <- as.data.frame(bank)
# create train dataframe and test dataframe
train_df <- bank[index,]
test_df <- bank[-index,]

#Relabel values of y to factor (1 = yes, 0 = no), because Logistic Regression needs factors for the outcome
train_df$y[train_df$y == 1] <- "yes"
train_df$y[train_df$y == 0] <- "no"

test_df$y[test_df$y == 1] <- "yes"
test_df$y[test_df$y == 0] <- "no"

#Convert outcome variable to type factor with function as.factor()
train_df$y <- as.factor(train_df$y)
test_df$y <- as.factor(test_df$y)

#Specify type of method, in this case cross validation, and number of folds will be 10. 
control <- trainControl(method = "cv", number = 10, savePredictions = "all",
                        classProbs = TRUE)
#Set random seed for folds
set.seed(7)

#Specify logistic regression model with method 'glm'. 

model1 <- train(y ~., data = train_df, method = 'glm', family = binomial,
                trControl = control)

```

After the cross validation is done, we need to see how well our improved model performs on test data. We use the predict() function to store the predictions generated by our model in predictions.

```{r include=FALSE}
#Predict outcome using trained model with test data.
predictions <- predict(model1, newdata = test_df)
```

Now, we use the confusionMatrix() function from the caret package to generate our accuracy.
```{r echo=FALSE}
confusionMatrix(data = predictions, test_df$y)

```


As we can see, the Logisitic Regression Model with and without cross validation has ~90-91% accuracy, but we need to add some form of bias because this model is overfit to this certain dataset. If we wanted to generalize this data to other banks in Portugal, we would need to compensate for the overfittness of this model to this dataset. 

This is where Lasso Regularization comes in.

#### Lasso Regularization


Lasso Regularization for Logistic Regression is done by minimizing this Cost Function:


$$J(\Theta) = \frac{1}{m}\Sigma^m_{i=1}[-y^{i}log(h_\Theta(x^{i}))-(1-y^{i})log(1-(h_\Theta(x^i))]+\frac{\lambda}{2m}\Sigma^n_{j=1}|\Theta_j|$$



First, we build a matrix for all the features of the dataset, which we'll call x, and a vector for the response variable, we'll call y, which consists of 1's and 0's for Yes and No.

```{r include=FALSE}
x <- model.matrix(y~., train_df)

y<- ifelse(train_df$y == "yes", 1,0)

```

In Lasso Regularization, there is a tuning paramater called Lambda in which we need to find the minimal value of, through cross validation. The minimum lambda will give us the most parsimonious model, but it will also generate a model which generally overfits. So we will pick the lambda which falls one standard error away from the minimum value in order to choose the simplest model without overfitting.


```{r include=FALSE}
#Cross Validation for lambda parameter
cv.out <- cv.glmnet(x,y,alpha=1, family="binomial", type.measure = "mse")
#Show log plot
plot(cv.out)
#Minimum lambda
lambda_min <- cv.out$lambda.min
#Lambda value that gives simplest model but also lies within one standard error of the optimal value of lambda.
lambda_1se <- cv.out$lambda.1se


```

From this graph, we see that the lambda value between -8 and -7 will give us the most minimal model, but one standard error away from that value will give us the lambda value near -5, which will give us the minimal model with the least overfitting. 

```{r include=FALSE}
lambda_min 
#Lambda value that gives simplest model but also lies within one standard error of the optimal value of lambda.
lambda_1se 

```


```{r echo=FALSE}
#Show coefficients of lassomodel
coef(cv.out, s = lambda_1se)

```
From the list of coefficients for the lasso model, we see that it selected 12 features that were the most significant on the model.

We now use our test data to see how accurate our model is.
```{r echo=FALSE}
x_test <- model.matrix(y~., test_df)
lasso_prob <- predict(cv.out, newx = x_test, s = lambda_1se, type = "response")
lasso_predict<- rep("no", nrow(test_df))
lasso_predict[lasso_prob>.5]<- "yes"
#confusion matrix
table(pred= lasso_predict, true = test_df$y)

mean(lasso_predict == test_df$y)



```

As we see, the accuracy of this model has decreased because we introduced some bias with the Lasso Regularization method. In order for this model to be applied to other data sets, we will sacrifice accuracy for the sake of not overfitting the model.



### Random Forest


Another method we can use on this data is the Random Forest Method. 

Using Random Forest gives us a vast amount of decision trees in which a model is made from the culmination of them. Using the ranodmForest library package, we can create the model in R. We first subset our data just like we previously did for the Logistic Regression model, with 70% of the data lumped into a Training Set, and 30% of our data lumped into the Test set. 

```{r include=FALSE}
library(randomForest)   # random forest methodology
bank.data = na.omit(bank.data)
bank.data$y = as.factor(bank.data$y)
dataset.size = floor(nrow(bank.data)*0.70)
index <- sample(1:nrow(bank.data),size= dataset.size)
training <- bank.data[index,]
test <- bank.data[-index,]
```
We then use the function "randomForest()" and train it with the previously created training set, which will create our random forest model to use for predictions. 

```{r include=FALSE}
rf <- randomForest(y~., data = training, importance = T)

```

After this is made, we can plot to see how many trees are necessary for our model

```{r echo=FALSE}
plot(rf)
```

This graph shows us that around 100-200 trees is where our model has minimum error

Now, let's confirm the variables of importance for the dataset using the varImpPlot() function

```{r echo=FALSE}
varImpPlot(rf)
```
As we can see, duration ranks number 1 most important variable in this dataset, which from eyeballing the data, we see the strong relationship between a phone calls duration and the result of a term deposit being made. This graph confirms this notion. 

Now, let's see how accurate this model is after feeding the model our test data. We will use the caret package to create the confusion matrix.

```{r echo=FALSE}
library(caret)
rftest <- predict(rf, newdata = test)
confusionMatrix(data = rftest, test$y)

```

From this confusion matrix, we can see that the accuracy was at 91.45%, and the Kappa value was at .53, which makes for a moderate model. This model gives us around the same accuracy as the logistic regression model. 



### Support Vector Machine(SVM)
As we know, linear regression explores the linear relationship between predictors and the corresponding variable($y$ here). However, for the data collected in the real world, the relationship between predictors and the response is always quite complex and non-linear. The bank data for this project contains 20 variables as predictors and it's trivial that the relationship between those variables and $y$ is non-linear and hard to explore. 

Thus, supporting vector machines is proposed to characterize such a complex non-linear relationship. To solve this problem, the input data is mapped into a higher dimensional space. By applying Kernel functions, a nonlinear problem in the lower dimensional space has been transferred into a linear one and thus an optimal separating hyper-plane can be learned. Actually, the hyper-plane is a boundary plane to perform the classification task. 

#### SVM full model
We use all the 20 variables to predict the response $y$ by the SVM algorithm. Since the SVM model can only deal with numerical data, first we convert all characteristic variables into numerical ones. Then we seperate the data into training and testing set and the training set accounts for 70\%. 

Since there is randomness in the training set generation part and also in the SVM training part, we apply set.seed() to fix the result. After training, the prediction accuracy on the test set is 90.928\%. 
```{r include=FALSE}
bank <- read.csv("/Users/yulujin/Dropbox/STA141/bank-additional-full.csv", sep=";")
head(bank)
library(dplyr)
library(factoextra)
library("e1071")
bank = na.omit(bank)
# data preparation
bank <- bank %>% mutate(marital = ifelse(as.character(marital) =="married", 1, 0))
bank <- bank %>% mutate(housing = ifelse(as.character(housing) =="yes", 1, 0))
bank <- bank %>% mutate(loan = ifelse(as.character(loan) =="yes", 1, 0))
bank <- bank %>% mutate(y = ifelse(as.character(y) =="yes", 1, 0))
bank$job <- as.numeric(factor(bank$job, levels = as.character(unique(bank$job))))
bank$education <- as.numeric(factor(bank$education, levels = as.character(unique(bank$education))))
bank$default <- as.numeric(factor(bank$default, levels = as.character(unique(bank$default))))
bank$contact <- as.numeric(factor(bank$contact, levels = as.character(unique(bank$contact))))
bank$month <- as.numeric(factor(bank$month, levels = as.character(unique(bank$month))))
bank$day_of_week <- as.numeric(factor(bank$day_of_week, levels = as.character(unique(bank$day_of_week))))
bank$poutcome <- as.numeric(factor(bank$poutcome, levels = as.character(unique(bank$poutcome))))

# train&test set
head(bank)
set.seed(1)
dataset.size = floor(nrow(bank)*0.70)
index <- sample(1:nrow(bank),size= dataset.size)
training <- bank[index,]
test <- bank[-index,]
dim(test)
```

```{r echo=FALSE}
set.seed(2)
# SVM
SVM <- svm(y~age+job+marital+education+housing+loan+contact+month+day_of_week+duration
           +campaign+pdays+previous+poutcome+emp.var.rate+cons.price.idx+cons.conf.idx
           +euribor3m+nr.employed+default, data = training, importance= T,type="C-classification")
SVM
result <- data.frame(test$y, predict(SVM,test[,1:20],type = "response"))
tab <- table(result)
tab
(10725+511)/(12357)
```

#### SVM by Variable Category
In the context of the business problem of predicting the success of telemarketing in getting a client to sign on to a long-term deposit based on a limited number of variables, we may encounter the case where the bank is not able to collect or access the full range of variables. In addition to feature selection using other methods, we may also attempt to predict $y$ based on different categories of information the bank may have. We consider life information (age, job, marital status, and education level), bank information (whether or not the client has a housing loan, personal loan, or credit in default), last contact information (number of times contacted during this campaign, number of days between the most recent two campaigns, number of times previously contacted, and outcome of the previous campaign for a particular client), and social and economic context at the time (employment variation rate, consumer price index, euribor 3 month rate, and number of employees).\
We first load and encode the data and then train four SVM models by separating the variables by category.

```{r, echo = FALSE}
# life situation: age, job, marital, education
set.seed(3)
svm_life <- svm(y~age+job+marital+education,
                data = training, importance= T, type="C-classification")
# bank related: housing loan, personal loan, credit in default
set.seed(4)
svm_bank <- svm(y~housing+loan+default,
                data = training, importance= T, type="C-classification")
# last contact related
set.seed(5)
svm_contact <- svm(y~campaign+pdays+previous+poutcome,
                   data = training, importance= T, type="C-classification")
# social and economic context attribute related
set.seed(6)
svm_econ <- svm(y~emp.var.rate+cons.price.idx+cons.conf.idx+euribor3m+nr.employed, 
                data = training, importance= T, type="C-classification")

```

We test our models using the test set containing 30% of the data, which we feed into the model and use the results to compute accuracy.

```{r, echo = FALSE}
# predicting from models using test
result_life <- data.frame(test$y, predict(svm_life, test[,1:20], type = "response"));
result_bank <- data.frame(test$y, predict(svm_bank, test[,1:20], type = "response"));
result_contact <- data.frame(test$y, predict(svm_contact, test[,1:20], type = "response"));
result_econ <- data.frame(test$y, predict(svm_econ, test[,1:20], type = "response"));

# function to get (accurately classified)/(total samples)
get_acc <- function(result_temp) {
  table_temp <- table(result_temp)
  return(sum(diag(table_temp))/sum(table_temp))
}

# implement get accuracy function
life_acc <- get_acc(result_life)
bank_acc <- get_acc(result_bank)
contact_acc <- get_acc(result_contact)
econ_acc <- get_acc(result_econ)

# show accuracy
life_acc
bank_acc
contact_acc
econ_acc
```

Our results are that life information and bank information have approximately 88.8\% accuracy, while last contact information has 89.6\% and socio-economic context has 89.0\%. So if the bank were able to use only one of these categories, it would have the best, if only marginally better, accuracy by using last contact information as their predictor. This is consistent with our previous findings that duration of phone call had the highest relative importance as a predictor, since the highest accuracy among the categories belongs to that of last contact information.

#### Cross Validation for SVM

To get the best possible parameters for SVM, we tune the model. To accommodate the function requirements, we use a different encoding in one aspect: we encode the output $y$ as factor levels instead of numeric binary or continuous. Our data is appropriately formatted for our needs, and we may perform cross validation. In this case, we use k-fold cross validation with $k=10$.

```{r eval=FALSE, include=FALSE}
# cross validation for svm
set.seed(7)
svm_tuned <- tune(svm, y~., data=training, tunecontrol = tune.control(cross=10))
summary(svm_tuned)
svm_cvd <- svm_tuned$best.model

svm_cvd_result <- table(test$y, predict(svm_cvd, test[,1:20], type = "response"))
svm_cvd_result
sum(diag(svm_cvd_result))/(sum(svm_cvd_result))
```

The result of performing 10-fold cross validation for SVM on this data set is a tuned model that has an accuracy of 90.92\% when tested on the test set. This is not an increase from our model that did not include cross validation. However, this may be accounted for by a possible decrease in over fitting, meaning that, while the accuracy for this particular test set is not better, the cross-validated model will generalize better.


### Principal component analysis(PCA)
Principal component analysis(PCA) is widely used for dimensionality reduction. PCA computes the principal components and then the data points can be projected onto the first few principal components to obtain lower-dimensional data while preserving as much of the data's variation as possible. Moreover, it also eexplores the data and tells us the most important variables in the dataset. 

We perform PCA on the dataset. By the summary result listed below, we have that the first 13 principal components give a cumulative proportion of 0.89615, which well represent the data.  
```{r echo=FALSE}
library(factoextra)
pca <- prcomp(bank[,-21], center = TRUE,scale. = TRUE)
summary(pca)
```
The screen plot of principal components is shown below. The y-axis explains the percentage of explained variance for each principal component and we see that the first 8 principal components have a percentage over 5%\. 
```{r echo=FALSE}
fviz_eig(pca)
```

By calculating the eigenvalue corresponding to each principal component, we have the following result. We would eliminate any variable that didn’t have an eigenvalue greater than 1. The idea behind this is that if the eigenvalue is less than 1, then the component accounts for less variance than a single variable contributed. Thus, we view the first 8 principal components as important ones. 
```{r echo=FALSE}
pca$sdev^2
```

By summarizing the first 8 principal components, we see that several variables are less significant than others, such as "contact", "day_of_week", "month", since they contribute little in the first 8 principal components. We then drop those variables and train the SVM model based on the same training and testing set. The test accuracy is then 90.807\%. Thus, there is no big difference between this model and the previous one trained by all the variables, which indicates that the dropped variables do not have much effect in the prediction process. 
```{r eval=FALSE, include=FALSE}
set.seed(8)
SVM <- svm(y~age+job+marital+education+housing+loan+duration
           +campaign+pdays+previous+poutcome+emp.var.rate+cons.price.idx+cons.conf.idx
           +euribor3m+nr.employed+default, data = training, importance= T,type="C-classification")
SVM
result_n <- data.frame(test$y, predict(SVM,test[,1:20],type = "response"))
tab_n <- table(result_n)
tab_n
(10730+491)/(12357)
```



### Isolation Forest
Using isolation forest, we compute the number of outliers in the data set. Since the output is not a logical value, we use a threshold of 0.5, which corresponds to average outlierness, to identify outliers. If a sample has greater than 0.5, or greater than average outlierness, then we mark it as an outlier.

```{r include=FALSE}
bank1 <- bank.data
bank1 = na.omit(bank1)
library(isotree)
# fit isolation forest model
set.seed(9)
iso <- isolation.forest(bank1, ntrees = 100, output_score = TRUE)
iso
# count outliers
sum(iso$scores >= 0.5)
# get percentage of data that is outliers
sum(iso$scores >= 0.5)/dim(bank1)[1]
```

We find that, in our data set with over 40,000 entries, we have 748 outliers, or that 1.82\% of the data has greater than average likelihood of being an outlier.


## Results
In this project, we perform logistic regression, random forest, SVM and their generalized models on the bank data. The results of different models are shown in the table. Therefore, we see that both logistic regression and random forest have better prediction accuracy on the test set. However, there is no signifcant difference in the performance of different models and all considered methods have acceptable prediction accuracy.
```{r include=FALSE}
library(xtable)
Model <- c("Logistic regression", "Random forest", "SVM", "SVM with PCA")
Accuracy <- c(0.913, 0.913,0.909,0.908)
Mat <- rbind(Model, Accuracy)
#xtable(Mat)

```
```{r echo=FALSE}
Mat
```


## Extra Credit
Other than logistic regression and random forest, we also run SVM to do the classification. For logistic regression and SVM, we perform cross validation. Moreover, for logistic regression, lasso regularization is applied to construct a sparse model. To analyze the outlier, we perform the isolation forest algorithm. 
